## "Attention Is All You Need"
- 2017년 Google Brain 팀에서 발표한 획기적인 논문입니다.

### 1. 트랜스포머(Transformer) 아키텍처 소개:
- 기존의 순환 신경망(RNN)이나 합성곱 신경망(CNN)을 사용하지 않고, 오직 어텐션(attention) 메커니즘만을 사용한 새로운 신경망 구조를 제안했습니다.

### 2. 자기 주의(Self-Attention) 메커니즘
- 입력 시퀀스의 각 요소가 다른 모든 요소와 어떻게 관련되는지를 계산합니다.
- 이를 통해 긴 거리의 의존성을 효과적으로 포착할 수 있습니다.

### 3. 멀티헤드 어텐션(Multi-Head Attention)
- 여러 개의 어텐션을 병렬로 수행하여 다양한 관점에서 정보를 추출합니다.

### 4. 인코더-디코더 구조
- 인코더는 입력 시퀀스를 처리하고, 디코더는 출력 시퀀스를 생성합니다.
- 두 부분 모두 트랜스포머 블록들로 구성됩니다.
  
### 5. 위치 인코딩(Positional Encoding)
- 순서 정보를 보존하기 위해 입력 임베딩에 위치 정보를 추가합니다.

### 6. 성능
- 기계 번역 작업에서 기존 모델들을 뛰어넘는 성능을 보여주었습니다.
- 학습 속도도 크게 향상되었습니다.

이 논문은 자연어 처리 분야에 큰 영향을 미쳤으며, BERT, GPT 등 현대적인 언어 모델의 기초가 되었습니다.

#### [▶︎ PPT 보러가기](https://github.com/hwd0ng/DL_Binary_classification/blob/main/%E1%84%8E%E1%85%B5%E1%84%90%E1%85%A1vs%E1%84%89%E1%85%A1%E1%84%8C%E1%85%A1_%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B3%E1%86%B8.ipynb)
